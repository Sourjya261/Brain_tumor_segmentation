{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6578224,"sourceType":"datasetVersion","datasetId":3798716},{"sourceId":6692144,"sourceType":"datasetVersion","datasetId":3858639}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# Define the paths to the two slice folders\ndata_dir1 = '/kaggle/input/brats-2020-slices/slices-20230925T215308Z-001/slices'\ndata_dir2 = '/kaggle/input/brats-2020-slices/slices-20230925T215308Z-002/slices'\n\n# Get a list of image file names from both folders\nimage_files1 = os.listdir(os.path.join(data_dir1, 'images'))\nimage_files2 = os.listdir(os.path.join(data_dir2, 'images'))\n\n# Initialize empty lists to store image and mask paths\nimage_paths = []\nmask_paths = []\n\n# Loop through image files from slices1 and check for corresponding masks\nfor image_file in image_files1:\n    image_path = os.path.join(data_dir1, 'images', image_file)\n    mask_file = image_file.replace(\"_image.npy\", \"_mask.npy\")\n    mask_path = os.path.join(data_dir1, 'masks', mask_file)\n    \n    image_paths.append(image_path)\n    # Check if the corresponding mask file exists in slices1->masks\n    if os.path.exists(mask_path):\n        #image_paths.append(image_path)\n        mask_paths.append(mask_path)\n    else:\n        #image_paths.append(image_path)\n        mask_path = os.path.join(data_dir2, 'masks', mask_file)\n        mask_paths.append(mask_path)\n        \n\n# Loop through image files from slices2 and check for corresponding masks\nfor image_file in image_files2:\n    image_path = os.path.join(data_dir2, 'images', image_file)\n    mask_file = image_file.replace(\"_image.npy\", \"_mask.npy\")\n    mask_path = os.path.join(data_dir2, 'masks', mask_file)\n    \n    image_paths.append(image_path)\n    # Check if the corresponding mask file exists in slices2->masks\n    if os.path.exists(mask_path):\n        #image_paths.append(image_path)\n        mask_paths.append(mask_path)\n    else:\n       # image_paths.append(image_path)\n        mask_path = os.path.join(data_dir1, 'masks', mask_file)\n        mask_paths.append(mask_path)\n        \n        \n\n# Create a DataFrame to store the paths\ndata = pd.DataFrame({'image_path': image_paths, 'mask_path': mask_paths})\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:25:28.260878Z","iopub.execute_input":"2023-10-14T08:25:28.261248Z","iopub.status.idle":"2023-10-14T08:25:40.190900Z","shell.execute_reply.started":"2023-10-14T08:25:28.261219Z","shell.execute_reply":"2023-10-14T08:25:40.189935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to check if a mask has less than 1% positive pixels\nimport numpy as np\ndef has_few_positive_pixels(mask_path, threshold=0.05):\n    mask = np.load(mask_path)\n    positive_pixels = np.sum(mask[:,:,1] > 0)\n    total_pixels = mask.size\n    return (positive_pixels / total_pixels) < threshold\n\ndef has_positive_pixels(mask_path, threshold=0.05):\n    mask = np.load(mask_path)\n    positive_pixels = np.sum(mask[:,:,0] > 0)\n    if positive_pixels>0 :\n        return True\n    else :\n        return False\n# Apply the function to the mask_path column\ndata['has_few_positive_pixels'] = data['mask_path'].apply(has_positive_pixels)\n\n# Filter rows based on the condition\ndata = data[~data['has_few_positive_pixels']]\n\n# Reset the index if needed\ndata.reset_index(drop=True, inplace=True)\n\n# Remove the temporary column\ndata.drop('has_few_positive_pixels', axis=1, inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:25:41.430718Z","iopub.execute_input":"2023-10-14T08:25:41.431082Z","iopub.status.idle":"2023-10-14T08:26:08.843978Z","shell.execute_reply.started":"2023-10-14T08:25:41.431054Z","shell.execute_reply":"2023-10-14T08:26:08.842877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(data['image_path'])","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:26:08.845710Z","iopub.execute_input":"2023-10-14T08:26:08.846045Z","iopub.status.idle":"2023-10-14T08:26:08.853534Z","shell.execute_reply.started":"2023-10-14T08:26:08.846015Z","shell.execute_reply":"2023-10-14T08:26:08.852480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data['image_path'][1])\nprint(data['mask_path'][1])","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:28:29.690938Z","iopub.execute_input":"2023-10-14T08:28:29.691364Z","iopub.status.idle":"2023-10-14T08:28:29.698271Z","shell.execute_reply.started":"2023-10-14T08:28:29.691315Z","shell.execute_reply":"2023-10-14T08:28:29.696989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot images and masks\nimport matplotlib.pyplot as plt\ndef plot_images_masks(images, masks):\n    num_samples = images.shape[0]\n\n    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 10))\n    for i in range(num_samples):\n        axes[i, 0].imshow(images[i],cmap='bone',interpolation='bicubic')\n        axes[i, 0].set_title('Image')\n        axes[i, 0].axis('on')\n\n        axes[i, 1].imshow(masks[i],cmap='bone',interpolation='bicubic')\n        axes[i, 1].set_title('Mask')\n        axes[i, 1].axis('on')\n\n        print(np.max(images[i]),np.max(masks[i]),masks[i].shape)\n\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:28:36.426977Z","iopub.execute_input":"2023-10-14T08:28:36.427324Z","iopub.status.idle":"2023-10-14T08:28:36.434812Z","shell.execute_reply.started":"2023-10-14T08:28:36.427297Z","shell.execute_reply":"2023-10-14T08:28:36.433567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Define data generator parameters\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=True,\n    vertical_flip=True,\n    fill_mode='constant'\n)\n\n# Function to load and preprocess images and masks\ndef load_and_preprocess(paths):\n    images = []\n    masks = []\n    for img_path, mask_path in zip(paths['image_path'], paths['mask_path']):\n        img = np.load(img_path).astype(np.float32)\n        mask = np.load(mask_path).astype(np.float32)\n        \"\"\"mask[:,:,0]=(mask[:,:,0]>0).astype(int)\n        mask[:,:,1]=(mask[:,:,1]>0).astype(int)\n        mask[:,:,2]=(mask[:,:,2]>0).astype(int)\n        new_channel = np.zeros((128, 128, 1), dtype=np.float32)\n        mask_4_channel = np.concatenate((new_channel,mask), axis=-1)\"\"\"\n        mask_=(mask[:,:,1]>0).astype(int)\n        mask_=np.expand_dims(mask_,axis=-1)\n        \n        images.append(img)\n        masks.append(mask_)\n    return np.array(images), np.array(masks)\n\n# Function to apply augmentations to both images and masks\ndef apply_augmentation(images, masks):\n    seed = np.random.randint(1, 1000)  # Generate a random seed for augmentations\n    image_gen = datagen.flow(images, batch_size=len(images), seed=seed)\n    mask_gen = datagen.flow(masks, batch_size=len(masks), seed=seed)\n    return next(image_gen), next(mask_gen)\n\n# Generate augmented batches of data with synchronized augmentations\ndef generate_data_generator(data, batch_size):\n    while True:\n        batch_indices = np.random.choice(len(data), batch_size)\n        batch_data = data.iloc[batch_indices]\n        x, y = load_and_preprocess(batch_data)\n        x_augmented, y_augmented = apply_augmentation(x, y)\n        yield x_augmented, y_augmented\n\n# Example usage\nbatch_size = 4\ntrain_data = generate_data_generator(data, batch_size)\n\n# Plot the batch\nbatch_images, batch_masks = next(train_data)\nplot_images_masks(batch_images, batch_masks)\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:28:42.565042Z","iopub.execute_input":"2023-10-14T08:28:42.565438Z","iopub.status.idle":"2023-10-14T08:28:44.138993Z","shell.execute_reply.started":"2023-10-14T08:28:42.565407Z","shell.execute_reply":"2023-10-14T08:28:44.138195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epsilon = 1e-5\nsmooth = 1\n\ndef iou(targets, inputs, smooth=1e-6):\n    \n    #flatten label and prediction tensors\n    class_num = 2\n    for i in range(class_num):\n        targets_f= K.flatten(targets[:,:,:,i])\n        inputs_f = K.flatten(inputs[:,:,:,i])\n       \n\n        intersection = K.sum((targets_f*inputs_f))\n        total = K.sum(targets_f) + K.sum(inputs_f)\n        union = total - intersection\n        loss = (intersection + smooth) / (union + smooth)\n        if i == 0:\n            total_loss = loss\n        else:\n            total_loss = total_loss + loss\n    total_loss = total_loss / class_num\n    return total_loss\n\ndef iou_tc(targets, inputs, smooth=1e-6):\n    \n    #flatten label and prediction tensors\n    targets_f = K.flatten(targets[:,:,:,1])\n    inputs_f = K.flatten(inputs[:,:,:,1])\n    \n    intersection = K.sum((targets_f * inputs_f))\n    total = K.sum(targets_f) + K.sum(inputs_f)\n    union = total - intersection\n    \n    IoU = (intersection + smooth) / (union + smooth)\n    return IoU\n\ndef tversky(y_true, y_pred):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\ndef focal_tversky(y_true,y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1-pt_1), gamma)\n\ndef tversky_loss(y_true, y_pred):\n    return 1 - tversky(y_true,y_pred)\n\nfrom keras import backend as K\n\ndef dice_coeff(y_true, y_pred, smooth=1):\n    y_true_flatten = K.flatten(y_true)\n    y_pred_flatten = K.flatten(y_pred)\n    intersection = K.sum(y_true_flatten * y_pred_flatten)\n    dice = (2. * intersection + smooth) / (K.sum(y_true_flatten) + K.sum(y_pred_flatten) + smooth)\n    return dice\ndef dice_loss(y_true, y_pred, smooth=1):\n    return  1- dice_coeff(y_true, y_pred, smooth=1)\n\ndef FocalLoss(targets, inputs, alpha=0.8, gamma=2):    \n    \n    inputs = K.flatten(inputs)\n    targets = K.flatten(targets)\n    \n    BCE = K.binary_crossentropy(targets, inputs)\n    BCE_EXP = K.exp(-BCE)\n    focal_loss = K.mean(alpha * K.pow((1-BCE_EXP), gamma) * BCE)\n    \n    return focal_loss\n\ndef total_loss(y_true, y_pred):\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    return dice_loss(y_true, y_pred, smooth=1) + FocalLoss(y_true, y_pred, 0.25,2)\n\ndef precision(y_true, y_pred):\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives / (predicted_positives + K.epsilon())\n        return precision\n\n    \n# Computing Sensitivity      \ndef sensitivity(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    return true_positives / (possible_positives + K.epsilon())\n\n\n# Computing Specificity\ndef specificity(y_true, y_pred):\n    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n    return true_negatives / (possible_negatives + K.epsilon())","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:29:03.090283Z","iopub.execute_input":"2023-10-14T08:29:03.090682Z","iopub.status.idle":"2023-10-14T08:29:03.108050Z","shell.execute_reply.started":"2023-10-14T08:29:03.090653Z","shell.execute_reply":"2023-10-14T08:29:03.106526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv2D,Conv2DTranspose, BatchNormalization, Activation, MaxPool2D, UpSampling2D, Concatenate, Input, ZeroPadding2D,Dropout,Dense,MaxPooling2D,Reshape,Multiply,GlobalAveragePooling2D,AveragePooling2D,Lambda\nfrom tensorflow.keras.models import Model\nfrom sklearn.decomposition import NMF\n\ndef batchnorm_relu(inputs):\n    x = BatchNormalization(axis=-1)(inputs)\n    x = Activation(\"relu\")(x)\n    return x\ndef ASPP(x, filter):\n    shape = x.shape\n\n    y1 = AveragePooling2D(pool_size=(shape[1], shape[2]))(x)\n    y1 = Conv2D(filter, 1, padding=\"same\")(y1)\n    y1 = BatchNormalization()(y1)\n    y1 = Activation(\"relu\")(y1)\n    y1 = UpSampling2D((shape[1], shape[2]), interpolation=\"bilinear\")(y1)\n\n    y2 = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(x)\n    y2 = BatchNormalization()(y2)\n    y2 = Activation(\"relu\")(y2)\n\n    y3 = Conv2D(filter, 3, dilation_rate=3, padding=\"same\", use_bias=False)(x)\n    y3 = BatchNormalization()(y3)\n    y3 = Activation(\"relu\")(y3)\n\n    y4 = Conv2D(filter, 3, dilation_rate=5, padding=\"same\", use_bias=False)(x)\n    y4 = BatchNormalization()(y4)\n    y4 = Activation(\"relu\")(y4)\n\n    y5 = Conv2D(filter, 3, dilation_rate=7, padding=\"same\", use_bias=False)(x)\n    y5 = BatchNormalization()(y5)\n    y5 = Activation(\"relu\")(y5)\n\n    y = Concatenate()([y1, y2, y3, y4, y5])\n\n    y = Conv2D(filter, 1, dilation_rate=1, padding=\"same\", use_bias=False)(y)\n    y = BatchNormalization()(y)\n    y = Activation(\"relu\")(y)\n\n    return y\n\n\ndef GCSE(input_tensor, ratio=8, i=0):\n    \n  \n    # Compute the global statistics (mean and std deviation) along the channel axis.\n    mean = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)\n    std = tf.math.reduce_std(input_tensor, axis=[1, 2], keepdims=True)\n    \n    # Compute channel-wise attention using a convolutional neural network.\n    attn_channel = tf.concat([mean, std], axis=-1)\n    attn_channel = tf.keras.layers.Conv2D(filters=input_tensor.shape[-1] // ratio, kernel_size=(1, 1), activation='relu',name=\"first_channel_attention_\"+str(i))(attn_channel)\n    attn_channel = tf.keras.layers.Conv2D(filters=input_tensor.shape[-1], kernel_size=(1, 1), activation='sigmoid',name=\"second_channel_attention_\"+str(i))(attn_channel)\n    \n    # Compute spatial attention using global information from the input tensor.\n    global_info = tf.reduce_mean(input_tensor, axis=-1, keepdims=True)\n    attn_spatial = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid',name=\"spatial_attention_\"+str(i))(global_info)\n    \n    # Combine channel-wise and spatial attention.\n    attn = tf.keras.layers.Multiply(name='combined_attention_'+str(i))([attn_channel,attn_spatial])\n    \n    # Multiply the input tensor by the learned attention weights.\n    output_tensor = tf.keras.layers.Multiply(name='SE_out_'+str(i))([input_tensor,attn])\n    \n    return output_tensor\n\ndef residual_block(inputs, num_filters):\n    \"\"\" Convolutional Layer \"\"\"\n    x = batchnorm_relu(inputs)\n    x = Conv2D(num_filters, 3, padding=\"same\", strides=1,kernel_initializer = \"he_normal\")(x)\n    x = Dropout(0.1)(x)\n    x = batchnorm_relu(x)\n    x = Conv2D(num_filters, 3, padding=\"same\", strides=1,kernel_initializer = \"he_normal\")(x)\n    #x = squeeze_excite_block(x,8)\n\n    \"\"\" Shortcut Connection \"\"\"\n    s = Conv2D(num_filters, 1, padding=\"same\", strides=1,kernel_initializer=\"he_normal\")(inputs)\n    x = x + s\n    return x\n\ndef decoder_block(inputs, skip_features, num_filters,i):\n    x = Conv2DTranspose(num_filters,2, strides=(2,2), kernel_initializer=\"he_normal\",padding = \"same\")(inputs)\n    x = Concatenate()([x, skip_features])\n    x = residual_block(x, num_filters)\n    x = GCSE(x,i=i)\n    return x\n\ndef build_resunet(input_shape):\n    inputs = Input(input_shape)\n    \n    #integrating augmentation directly into the model\n    #x = tf.keras.layers.RandomContrast(0.15)(inputs)\n    #x = tf.keras.layers.RandomTranslation(height_factor=0.2, width_factor=0.2)(x)\n    #x = tf.keras.layers.RandomFlip(\"horizontal_and_vertical\")(x)\n    #x_ = tf.keras.layers.RandomZoom(0.3)(x)\n    \n   \n\n    \n\n    \"\"\" Encoder 1 \"\"\"\n    x = Conv2D(32, 3, padding=\"same\", strides=1,kernel_initializer=\"he_normal\")(inputs)\n    x = batchnorm_relu(x)\n    x = Conv2D(32, 3, padding=\"same\", strides=1,kernel_initializer=\"he_normal\")(x)\n    s = Conv2D(32, 1, padding=\"same\", strides=1,kernel_initializer=\"he_normal\")(inputs)\n    \n    c1 = x + s\n    c1 =GCSE(c1,i=1)\n    p1 = MaxPooling2D((2,2))(c1)\n    #s1 = residual_block(c1,32)\n    s1 = c1\n\n    \"\"\" Encoder 2 and 3 \"\"\"\n    c2 = residual_block(p1, 64)\n    c2 = GCSE(c2,i=2)\n    p2 = MaxPooling2D((2,2))(c2)\n    #s2 = residual_block(c2,64)\n    s2 = c2\n    \n    c3 = residual_block(p2, 128)\n    c3 =GCSE(c3,i=4)\n    p3 = MaxPooling2D((2,2))(c3)\n    #s3 = residual_block(c3,128)\n    s3 = c3\n\n    c4 = residual_block(p3, 256)\n    c4 = GCSE(c4,i=5)\n    p4 = MaxPooling2D((2,2))(c4)\n    #s4 = residual_block(c4,256)\n    s4 = c4\n\n    \"\"\" Bridge \"\"\"\n    b = ASPP(p4,256)\n    #b = p4\n\n    \"\"\" Decoder 1, 2, 3 \"\"\"\n    d1 = decoder_block(b, s4, 256,i=6)\n    d2 = decoder_block(d1, s3, 128,i=7)\n    d3 = decoder_block(d2, s2, 64,i=8)\n    d4 = decoder_block(d3, s1, 32,i=9)\n\n    \"\"\" Classifier \"\"\"\n    outputs = Conv2D(1, 1, padding=\"same\", activation=\"sigmoid\")(d4)\n\n    \"\"\" Model \"\"\"\n    model = Model(inputs, outputs)\n    return model\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:21:19.211392Z","iopub.execute_input":"2023-10-14T09:21:19.211781Z","iopub.status.idle":"2023-10-14T09:21:19.234971Z","shell.execute_reply.started":"2023-10-14T09:21:19.211750Z","shell.execute_reply":"2023-10-14T09:21:19.233400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install -U segmentation-models\n%env SM_FRAMEWORK=tf.keras\nimport segmentation_models as sm","metadata":{"execution":{"iopub.status.busy":"2023-10-14T08:29:11.185478Z","iopub.execute_input":"2023-10-14T08:29:11.185828Z","iopub.status.idle":"2023-10-14T08:29:21.276894Z","shell.execute_reply.started":"2023-10-14T08:29:11.185800Z","shell.execute_reply":"2023-10-14T08:29:21.275533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compling model and callbacks functions\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.callbacks import CSVLogger\nmodel = build_resunet((128,128,3))\nadam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\nmodel.compile(optimizer = adam, \n                  loss = total_loss, \n                  metrics = [sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5),dice_coeff,precision,sensitivity,specificity,tf.keras.metrics.AUC()]\n                 )\n#callbacks\ncsv_logger = CSVLogger('/kaggle/working/training_log_tc', separator=',', append=False)\n\"\"\"earlystopping = EarlyStopping(monitor='val_loss',\n                              mode='min', \n                              verbose=1, \n                              patience=8\n                             )\"\"\"\n# save the best model with lower validation loss\ncheckpointer = ModelCheckpoint(filepath=\"/kaggle/working/seg_model_tc.h5\", \n                               verbose=1, \n                               save_best_only=True,\n                               save_weights_only=True\n                              )\n                              \nreduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                              mode='min',\n                              verbose=1,\n                              patience=3,\n                              min_delta=0.000001,\n                              factor=0.2\n                             )\ncallbacks = [checkpointer, reduce_lr, csv_logger]","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:21:23.621253Z","iopub.execute_input":"2023-10-14T09:21:23.621970Z","iopub.status.idle":"2023-10-14T09:21:24.682644Z","shell.execute_reply.started":"2023-10-14T09:21:23.621938Z","shell.execute_reply":"2023-10-14T09:21:24.681645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split the data into training and validation sets\ntrain_data, val_data = train_test_split(data, test_size=0.20, random_state=42)\n\n# Define batch size\nbatch_size = 8\n\n# Create data generators for training and validation sets\ntrain_generator = generate_data_generator(train_data, batch_size)\nval_generator = generate_data_generator(val_data, batch_size)\n\ntrain_steps_per_epoch = len(train_data) // batch_size\nval_steps_per_epoch = len(val_data) // batch_size\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:21:26.791846Z","iopub.execute_input":"2023-10-14T09:21:26.792527Z","iopub.status.idle":"2023-10-14T09:21:26.800392Z","shell.execute_reply.started":"2023-10-14T09:21:26.792496Z","shell.execute_reply":"2023-10-14T09:21:26.799012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(train_generator, \n                  steps_per_epoch=train_steps_per_epoch,\n                  epochs = 50, \n                  validation_data = val_generator,\n                  validation_steps=val_steps_per_epoch,\n                  callbacks = callbacks\n                 )","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:21:32.184665Z","iopub.execute_input":"2023-10-14T09:21:32.185030Z","iopub.status.idle":"2023-10-14T09:29:30.387711Z","shell.execute_reply.started":"2023-10-14T09:21:32.185001Z","shell.execute_reply":"2023-10-14T09:29:30.386607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = pd.read_csv('/kaggle/input/brats-slices-0-05-model-and-log-tc/training_log_tc', sep=',', engine='python')\n\n#hist=history\n\n############### ########## ####### #######\n\niou_score=history.history['iou_score']\nval_iou_score=history.history['val_iou_score']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\ntrain_dice=history.history['dice_coeff']\nval_dice=history.history['val_dice_coeff']\n\nepoch = np.arange(len(loss))\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\nplt.subplots_adjust(wspace=0.3)  # Adjust the space between subplots\n\n# Plot 1: Loss\naxes[0].plot(epoch, loss, 'b', label='Training Loss', linewidth=1.5)\naxes[0].plot(epoch, val_loss, 'r', label='Validation Loss', linewidth=1.5)\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training and Validation Loss')\naxes[0].legend()\n\n# Plot 2: IOU Scores\naxes[1].plot(epoch, iou_score, 'b', label='Training IOU', linewidth=1.5)\naxes[1].plot(epoch, val_iou_score, 'r', label='Validation IOU', linewidth=1.5)\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('IOU Score')\naxes[1].set_title('Training and Validation IOU Score')\naxes[1].legend()\n\n# Plot 3: Dice Coefficients\naxes[2].plot(epoch, train_dice, 'b', label='Training Dice Coeff', linewidth=1.5)\naxes[2].plot(epoch, val_dice, 'r', label='Validation Dice Coeff', linewidth=1.5)\naxes[2].set_xlabel('Epochs')f\naxes[2].set_ylabel('Dice Coefficient')\naxes[2].set_title('Training and Validation Dice Coefficient')\naxes[2].legend()\n\n# Customize tick marks and labels\nfor ax in axes:\n    ax.grid(True)\n    ax.set_xticks(np.arange(0, len(epoch), 5))  # Adjust the x-axis ticks\n    ax.set_yticks(np.arange(0, 1.1, 0.1))  # Adjust the y-axis ticks\n    ax.grid(which='both', linestyle='--', linewidth=0.5)\n\n# Display the plots\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:38:56.744446Z","iopub.execute_input":"2023-10-14T09:38:56.744984Z","iopub.status.idle":"2023-10-14T09:38:57.606062Z","shell.execute_reply.started":"2023-10-14T09:38:56.744953Z","shell.execute_reply":"2023-10-14T09:38:57.604806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for layer in model.layers:\n    print(layer.name)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:39:29.162044Z","iopub.execute_input":"2023-10-14T09:39:29.162427Z","iopub.status.idle":"2023-10-14T09:39:29.169192Z","shell.execute_reply.started":"2023-10-14T09:39:29.162394Z","shell.execute_reply":"2023-10-14T09:39:29.168043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_data_generator(data, batch_size):\n    while True:\n        batch_indices = np.random.choice(len(data), batch_size)\n        batch_data = data.iloc[batch_indices]\n        x, y = load_and_preprocess(batch_data)\n        #x_augmented, y_augmented = apply_augmentation(x, y)\n        #yield x_augmented, y_augmented\n        yield x,y","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:56:20.327572Z","iopub.execute_input":"2023-10-14T09:56:20.327975Z","iopub.status.idle":"2023-10-14T09:56:20.333746Z","shell.execute_reply.started":"2023-10-14T09:56:20.327946Z","shell.execute_reply":"2023-10-14T09:56:20.332549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_generator = generate_data_generator(val_data, batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T09:56:24.054829Z","iopub.execute_input":"2023-10-14T09:56:24.055183Z","iopub.status.idle":"2023-10-14T09:56:24.059426Z","shell.execute_reply.started":"2023-10-14T09:56:24.055157Z","shell.execute_reply":"2023-10-14T09:56:24.058424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = next(val_generator)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:51:57.286013Z","iopub.execute_input":"2023-10-14T10:51:57.286394Z","iopub.status.idle":"2023-10-14T10:51:57.328704Z","shell.execute_reply.started":"2023-10-14T10:51:57.286337Z","shell.execute_reply":"2023-10-14T10:51:57.327749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample[0][0:1].shape","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:50:18.251555Z","iopub.execute_input":"2023-10-14T10:50:18.251901Z","iopub.status.idle":"2023-10-14T10:50:18.258295Z","shell.execute_reply.started":"2023-10-14T10:50:18.251873Z","shell.execute_reply":"2023-10-14T10:50:18.257406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extract the input and the corresponding mask from the sample\ninput_sample = sample[0][0:1]  # Assuming the input is the first element in the sample\nmask_sample = sample[1][0:1]  # Assuming the mask is the second element in the sample\n\n# Predict the output using the trained model\noutput = model.predict(input_sample)>0.7\n\n# Plot the input, mask, and output\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1, 3, 1)\nplt.title('Input')\nplt.imshow(np.squeeze(input_sample), cmap='gray')  # Assuming the input is grayscale\nplt.axis('off')\n\nplt.subplot(1, 3, 2)\nplt.title('Mask')\nplt.imshow(np.squeeze(mask_sample), cmap='gray')  # Assuming the mask is grayscale\nplt.axis('off')\n\nplt.subplot(1, 3, 3)\nplt.title('Output')\nplt.imshow(np.squeeze(output), cmap='gray')  # Assuming the output is grayscale\nplt.axis('off')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:51:59.733090Z","iopub.execute_input":"2023-10-14T10:51:59.733863Z","iopub.status.idle":"2023-10-14T10:52:00.208060Z","shell.execute_reply.started":"2023-10-14T10:51:59.733830Z","shell.execute_reply":"2023-10-14T10:52:00.207199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"channel_wise_attention_model = Model(inputs=model.input, outputs=model.get_layer('second_channel_attention_2').output)\nspatial_attention_model = Model(inputs=model.input, outputs=model.get_layer('spatial_attention_2').output)\ncombined_attention_model = Model(inputs=model.input, outputs=model.get_layer('combined_attention_2').output)\nSE_out_model = Model(inputs=model.input, outputs=model.get_layer('SE_out_2').output)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:53:54.465671Z","iopub.execute_input":"2023-10-14T10:53:54.466045Z","iopub.status.idle":"2023-10-14T10:53:54.493191Z","shell.execute_reply.started":"2023-10-14T10:53:54.466018Z","shell.execute_reply":"2023-10-14T10:53:54.492250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SE_out = SE_out_model.predict(input_sample)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:53:59.486842Z","iopub.execute_input":"2023-10-14T10:53:59.487595Z","iopub.status.idle":"2023-10-14T10:53:59.726372Z","shell.execute_reply.started":"2023-10-14T10:53:59.487559Z","shell.execute_reply":"2023-10-14T10:53:59.725411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(SE_out[0,:,:,5])","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:54:02.418364Z","iopub.execute_input":"2023-10-14T10:54:02.419607Z","iopub.status.idle":"2023-10-14T10:54:02.681366Z","shell.execute_reply.started":"2023-10-14T10:54:02.419563Z","shell.execute_reply":"2023-10-14T10:54:02.680406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SE_out.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:07:02.330032Z","iopub.execute_input":"2023-10-14T10:07:02.330423Z","iopub.status.idle":"2023-10-14T10:07:02.337082Z","shell.execute_reply.started":"2023-10-14T10:07:02.330392Z","shell.execute_reply":"2023-10-14T10:07:02.335840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map = SE_out","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:00:16.257580Z","iopub.execute_input":"2023-10-14T11:00:16.258688Z","iopub.status.idle":"2023-10-14T11:00:16.262982Z","shell.execute_reply.started":"2023-10-14T11:00:16.258648Z","shell.execute_reply":"2023-10-14T11:00:16.261916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif len(feature_map.shape) == 4:\n    k = feature_map.shape[-1]\n    size=feature_map.shape[1]\n    image_belt = np.zeros((size,size*k))\n    for i in range(k//2):\n        feature_image = feature_map[0, :, :, i]\n        feature_image-= feature_image.mean()\n        feature_image/= feature_image.std ()\n        feature_image*=  64\n        feature_image+= 128\n        feature_image= np.clip(feature_image, 0, 255).astype('uint8')\n        image_belt[:,i * size : (i + 1) * size] = feature_image    \n\n\nscale = 100. / k\nplt.figure( figsize=(scale * k, scale) )\nplt.grid  ( False )\nplt.imshow( image_belt, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:00:35.095622Z","iopub.execute_input":"2023-10-14T11:00:35.095998Z","iopub.status.idle":"2023-10-14T11:00:35.826357Z","shell.execute_reply.started":"2023-10-14T11:00:35.095971Z","shell.execute_reply":"2023-10-14T11:00:35.825529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map = spatial_attention_model.predict(input_sample)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:54:39.385667Z","iopub.execute_input":"2023-10-14T10:54:39.386381Z","iopub.status.idle":"2023-10-14T10:54:39.611239Z","shell.execute_reply.started":"2023-10-14T10:54:39.386327Z","shell.execute_reply":"2023-10-14T10:54:39.610128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:54:42.090596Z","iopub.execute_input":"2023-10-14T10:54:42.090946Z","iopub.status.idle":"2023-10-14T10:54:42.097013Z","shell.execute_reply.started":"2023-10-14T10:54:42.090919Z","shell.execute_reply":"2023-10-14T10:54:42.095961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(feature_image):\n    feature_image-= feature_image.mean()\n    feature_image/= feature_image.std ()\n    feature_image*=  64\n    feature_image+= 128\n    feature_image= np.clip(feature_image, 0, 255).astype('uint8')\n    return feature_image[0,:,:,:]\nplt.imshow(process(feature_map))","metadata":{"execution":{"iopub.status.busy":"2023-10-14T10:54:44.895219Z","iopub.execute_input":"2023-10-14T10:54:44.896390Z","iopub.status.idle":"2023-10-14T10:54:45.157421Z","shell.execute_reply.started":"2023-10-14T10:54:44.896323Z","shell.execute_reply":"2023-10-14T10:54:45.156533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb_features  = combined_attention_model.predict(input_sample)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:04:48.227276Z","iopub.execute_input":"2023-10-14T11:04:48.228410Z","iopub.status.idle":"2023-10-14T11:04:48.472199Z","shell.execute_reply.started":"2023-10-14T11:04:48.228366Z","shell.execute_reply":"2023-10-14T11:04:48.471216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comb_features.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:05:00.105530Z","iopub.execute_input":"2023-10-14T11:05:00.106639Z","iopub.status.idle":"2023-10-14T11:05:00.113535Z","shell.execute_reply.started":"2023-10-14T11:05:00.106595Z","shell.execute_reply":"2023-10-14T11:05:00.112417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map = comb_features\nif len(feature_map.shape) == 4:\n    k = feature_map.shape[-1]\n    size=feature_map.shape[1]\n    image_belt = np.zeros((size,size*k))\n    for i in range(k//2):\n        feature_image = feature_map[0, :, :, i]\n        feature_image-= feature_image.mean()\n        feature_image/= feature_image.std ()\n        feature_image*=  64\n        feature_image+= 128\n        feature_image= np.clip(feature_image, 0, 255).astype('uint8')\n        image_belt[:,i * size : (i + 1) * size] = feature_image    \n\nscale = 100. / k\nplt.figure( figsize=(scale * k, scale) )\nplt.grid  ( False )\nplt.imshow( image_belt, aspect='auto')","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:06:03.633206Z","iopub.execute_input":"2023-10-14T11:06:03.634231Z","iopub.status.idle":"2023-10-14T11:06:04.306246Z","shell.execute_reply.started":"2023-10-14T11:06:03.634176Z","shell.execute_reply":"2023-10-14T11:06:04.305430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map= channel_wise_attention_model.predict(input_sample)","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:09:00.368388Z","iopub.execute_input":"2023-10-14T11:09:00.369079Z","iopub.status.idle":"2023-10-14T11:09:00.600982Z","shell.execute_reply.started":"2023-10-14T11:09:00.369047Z","shell.execute_reply":"2023-10-14T11:09:00.600008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_map.shape","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:09:09.064105Z","iopub.execute_input":"2023-10-14T11:09:09.064827Z","iopub.status.idle":"2023-10-14T11:09:09.071110Z","shell.execute_reply.started":"2023-10-14T11:09:09.064794Z","shell.execute_reply":"2023-10-14T11:09:09.069733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(feature_map[0,0,0,:])","metadata":{"execution":{"iopub.status.busy":"2023-10-14T11:10:58.968759Z","iopub.execute_input":"2023-10-14T11:10:58.969847Z","iopub.status.idle":"2023-10-14T11:10:59.211106Z","shell.execute_reply.started":"2023-10-14T11:10:58.969803Z","shell.execute_reply":"2023-10-14T11:10:59.210276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\ndef novel_se_block(input_tensor, ratio=4):\n    \"\"\"\n    Novel Squeeze-and-Excite Block with Dynamic Channel-wise and Spatial Attention.\n    \n    Args:\n        input_tensor: Input tensor to the SE block.\n        ratio: Reduction ratio for the hidden layers.\n    \n    Returns:\n        Output tensor with dynamic feature scaling.\n    \"\"\"\n    \n    # Compute the global statistics (mean and std deviation) along the channel axis.\n    mean = tf.reduce_mean(input_tensor, axis=[1, 2], keepdims=True)\n    std = tf.math.reduce_std(input_tensor, axis=[1, 2], keepdims=True)\n    \n    # Compute channel-wise attention using a convolutional neural network.\n    attn_channel = tf.concat([mean, std],axis=2)\n    print(attn_channel.shape)\n    attn_channel = tf.keras.layers.Conv2D(filters=input_tensor.shape[-1] // ratio, kernel_size=(1, 1), activation='relu')(attn_channel)\n    attn_channel = tf.keras.layers.Conv2D(filters=input_tensor.shape[-1], kernel_size=(1, 1), activation='sigmoid')(attn_channel)\n    \n    # Compute spatial attention using a convolutional neural network.\n    attn_spatial = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(input_tensor)\n    \n    # Combine channel-wise and spatial attention.\n    attn = attn_channel * attn_spatial\n    \n    # Multiply the input tensor by the learned attention weights.\n    output_tensor = input_tensor * attn\n    \n    return output_tensor","metadata":{"execution":{"iopub.status.busy":"2023-10-16T14:02:40.102546Z","iopub.execute_input":"2023-10-16T14:02:40.102893Z","iopub.status.idle":"2023-10-16T14:02:40.111241Z","shell.execute_reply.started":"2023-10-16T14:02:40.102866Z","shell.execute_reply":"2023-10-16T14:02:40.110066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_tensor = tf.keras.layers.Input(shape=(64, 64, 64))  # Example input shape\nse_output = novel_se_block(input_tensor)","metadata":{"execution":{"iopub.status.busy":"2023-10-16T14:02:43.032071Z","iopub.execute_input":"2023-10-16T14:02:43.032421Z","iopub.status.idle":"2023-10-16T14:02:43.746693Z","shell.execute_reply.started":"2023-10-16T14:02:43.032395Z","shell.execute_reply":"2023-10-16T14:02:43.745112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}